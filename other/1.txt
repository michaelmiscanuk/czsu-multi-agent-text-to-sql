2.1.2	Structured Data
My application wasn’t directly connected to any data from the beginning, so when I started the project my first task was to find interesting and useful data that I would be able to use for the kind of project in my mind, that is agentic flow that would allow us to communicate with structured and unstructured data.

After some research, data that I found was available for extraction by API from Czech Statistical Office (CZSU). Description of the API is provided on this website //link// Základní informace pro použití API DataStatu | Statistika. There are some nuances and complexities in this data making it a good fit for this kind of project. I will describe this in better detail, since understanding the data will allow us to work better with them later.

There are three layers that we must go through to extract all of the selections with the specific values. On the upper level, we have a list of all datasets available //link// data.csu.gov.cz/api/katalog/v1/sady . From this list we can construct links to the available selections in each dataset, for example //link// https://data.csu.gov.cz/api/katalog/v1/sady/OBY01PD/vybery . And each selection has a specific code that allows us to construct a link where we get data in a JSON-Stat format, for example here //link// data.csu.gov.cz/api/dotaz/v1/data/vybery/OBY01PDT01. 

In the following part I address a few key points regarding the data extraction and loading implementation and data complexities. More detailed descriptions can be found in the docstrings and comments in the code – which are written iteratively with AI assistance and my verification. 

There are 2 main scripts for data extraction. Script called datasets_selections_get_csvs.py is the main script that downloads and transforms all data to a CSV format. Here we use nested loops and link constructions to get the data. Each selection is loaded to the pandas DataFrame using available library function and then saved to a CSV, where we need to specify UTF-8 as encoding, because all data files contain Czech Characters. 

Here is a list of the main data complexities which make it challenging to work with them, manually or with AI:
•	Columns names, dimensional and metric values are in Czech Language
•	Amount of dataset selections, over 900 tables
•	Dataset can have similar selections, which can differ in dimension or metrics used, but otherwise have same focus area, for example selections about population
•	Long Data Format, with multiple metrics in one column
•	Columns names, dimensional and metric values are often sentences with additional notes in brackets
•	Czech descriptive abbreviations, e.g. HUZ
•	Tables often contain sets of records which are the total for other dimensional values, for example values for each region and values for the whole country in one file
•	One column with values, which contains different kinds of values based on metric name in another column, like counts, aggregations, percentages
•	Selections can have only some dimensional values present, for example, only one-year, or only one region present
•	Records can have missing values

Here is an example of one of the selections in CSV format with several records to have an idea of how this data looks like, here formatted as a table and translated to English. The selection shows the number of visitors in accommodation facilities.


Years	Czech Republic, Cohesion Regions, Regions	Indicator	Residence	value
2023	Czech Republic	Number of guests	Total	21977671
2023	Czech Republic	Number of guests	Residents	12418741
2023	Czech Republic	Number of guests	Non-residents	9558930
2023	Czech Republic	Number of overnight stays	Total	55842471
2023	Czech Republic	Number of overnight stays	Residents	32480862
2023	Czech Republic	Number of overnight stays	Non-residents	23361609
2023	Prague Capital City	Number of guests	Total	7442614
2023	Prague Capital City	Number of guests	Residents	1577633
2023	Prague Capital City	Number of guests	Non-residents	5864981
2023	Prague Capital City	Number of overnight stays	Total	16861664
2023	Prague Capital City	Number of overnight stays	Residents	2691291
2023	Prague Capital City	Number of overnight stays	Non-residents	14170373


Second script that is used for data handling, csvs_to_sqllite.py, stores all CSV files into one table in the local SQLite Database. Local database I have chosen for simplicity reasons, otherwise this is a less standard way, which requires, for example, compressing the .db file to be able to put it into git. Data is small enough and open, so the local database is sufficient for our use case and prototype solution. 


2.1.3	Metadata
Metadata are descriptive elements for our data. In our case, we are mainly concerned about descriptions of each selection or table. The main usage of this metadata is to use those descriptions to find best selection for the user query based on semantic search. Other usage is to display selections and their descriptions in the UI. 

In this section I go over the steps I did to prepare metadata for this semantic search. I created a semi-automatic process to get selection descriptions and store them into a vector database using several scripts. This can be streamlined to get the end-to-end flow, but for our purposes I left the self-contained scripts with separated logic since they are not used for updates, and the LLM step at the end can be expensive. Below i describe the process in more details from a conceptual view. Even more technical details can be found as comments and docstrings in individual scripts under provided links.

JSON-stat format contains both data and metadata for each selection. The script get_metadata_schemas_all_selections.py is used to extract the metadata from json-stat format and store them into a json format. Because we have a lot of selections, script is prepared in a way that will go over all datasets and selection in parallel with dynamic link building, this is done similarly to extraction of data. Each selection is then accessed using a REST API library, which returns a json, which is then stored as a python dictionary and the core of the code is that I simply select the necessary metadata keys excluding the data keys. The result is that for each selection a json file is created with its metadata named by its Selection ID. The main descriptive elements that we are going to use later are the label or description of the selection, column labels and distinct values in each non-value column.

Other similar and simplified script - extract_selection_descriptions_from_metadata.py, creates a CSV file with 2 columns: selection_code and description. Selection code is the ID of selection and description in this case is again taken from Json-stat and extended by temporal and regional information if present, here is an example of one record:

"OBY03T01";"Births - selected summary data. Time granularity: Year. Territory types: State, Region"

The next script I prepared can be used with minor adjustments in other places where we need to do several iterations over LLM to provide an answer to our prompt for each row in a table - dynamic_parallel_dataframe_llm_processor.py. In our case, we are taking previously extracted schemas and simple original descriptions of all selections, and we ask LLM to give us back the extended description that will be used to create vector embeddings for the semantic search to find the best selection matching the user query. 
Because prompting LLM many times over many records can be long and expensive depending on the size of data and LLM model chosen, I implemented several precocious. To reduce cost, I don't allow to send prompt for selection ID when we already have extended description for it, which means it was already processed before. Results are incrementally saved to the SQLLite database one record at time – this way if processing fails, it can be picked up when we execute the script again. This makes our script repeatable, which means we can run it as needed, and it will only add LLM answer to the records or IDs where it is missing. 
To speed up the process I use parallel processing to process several records in a table simultaneously. The problem here is that making too many API requests to the LLM model at once will result into limits reached. Both cases are controlled by config variables. Because running this script can take some time, I also added the progress monitoring in the terminal using TQDM package.
Prompt template //link// https://github.com/michaelmiscanuk/czsu-multi-agent-text-to-sql/blob/main/metadata/llm_selection_descriptions/PROMPT_TEMPLATE.txt     that is used to ask LLM to provide the extended description has placeholders for short description and schema. Those will be replaced in each iteration over our pandas dataframe by cell values in given columns. Full Prompt then will be provided to LLM. Answer from the LLM is written to the column extended_description. To keep those descriptions similar and standardized I provide a set of rules, output format and most importantly the example output structure.
For LLM I used Azure OpenAI service where I served several models to be used throughout our application. The one used here is GPT-4.1 model. Models are configured in my code in models.py. Returned extended selection descriptions therefore have well-structured textual format describing the focus of the dataset, listing all the columns and all distinct values in each column. Below is the reduced example of the LLM answer for one of the selections. Note that there is a mix of Czech and English language, which should not be a problem for our use here which is the creation of the vector embeddings.
This dataset focuses on "Narození" (Births). The data is collected at the "Kumulace čtvrtletí" (Cumulative Quarter) level and covers the territory types "ČR, kraje" (State, Region). The dataset includes the following columns: "Kumulace čtvrtletí", "ČR, kraje", and "Ukazatel".
For "Kumulace čtvrtletí", the available values are "Q1-Q3 2024".
 
For "ČR, kraje", the available values are:
- "Česko"
- "Hlavní město Praha"
- "Středočeský kraj"
 
For "Ukazatel", the available values are:
- "Narození"
- "Živě narození"
- "Živě narození - chlapci"

The data is organized hierarchically, with "Kumulace čtvrtletí" representing the time dimension, "ČR, kraje" representing the geographic dimension, and "Ukazatel" representing the metric or indicator dimension. This structure allows for semantic search and analysis of births data across different regions and time periods.
The next script create_and_load_chromadb.py can be used as a standalone runnable script, which is used to create and fill the ChromaDB with vector embeddings, but also can be used as a module to be loaded into other scripts providing custom functions to work with this ChromaDB for document retrieval functionality using semantic search. Here I describe the first part which belongs to metadata preparation, second part is covered in the agentic flow during the step of finding best selections based on user query.
In the code I am moving data from the previously prepared table in SQLite database with extended descriptions to the ChromaDB database. Script is done in a way that can be executed repeatably, which means that calls to LLM to generate embedding are done only for new or changed extended descriptions – using the upsert pattern. Main execution flow happens in upsert_documents_to_chromadb() function.  I start by getting all documents from SQLite and I generate a MD5 hash over extended description column. ChromaDB collection is then created if it does not exist, otherwise we just get a pointer to it. MD5 hash is then used to check if retrieved records are present in collection. Existing documents are filtered out and remaining are processed in batches to speed up the process.
ChromaDB is configured with HNSW set to space with cosine similarity, which is best suited for text embeddings. Embeddings are created using LLM model served in Azure OpenAI called text-embedding-3-large. This large model creates a vector of 3072 components for each chunk of text and has 8191 max token size. For this reason, I added a splitting function that divides the text into multiple chunks in a way that one chunk won’t exceed the limit.
The chunking strategy I used here is a simple division of text into equally sized chunks. The reason for this is that in most cases the extended description of selection won’t exceed the limit. If this is the case, some improvements can be made to chunking of our text. This I discuss more in possible improvements section. To get number of tokens I use the tiktoken tokenizer from OpenAI with cl100k_base encoding suitable for our embedding model. Then ceil division calculation of number of tokens in a text diving by max number of tokens is applied to find number of chunks that the text must be split into.
2.1.4	Unstructured Data
I have put section about unstructured data after metadata because there are some similarities in the processing of both. Previously we had extended descriptions of each dataset that we use to find best matching selections for the user prompt based on vector semantic search. When best description is found, we took the selections that this description is for, and this selection contains our data. In this case with unstructured data, the data itself, described in natural language, is our data that is searched based on vector semantic search to be closest to the user prompt. 
Data that I have chosen for our application comes with its own set of complexities, which again makes it ideal to get a feel to match some real-world scenarios. The PDF file I used is called “Statistical Yearbook of the Czech Republic”  //link https://csu.gov.cz/docs/107508/db2ef2f9-5b1f-82c2-5ebf-621acf94791d/32019824.pdf?version=1.2 // . It contains broad statistical overview information of industries in Czech economy. While there is some overlap with our tabular data, the two sources are designed to complement each other, expanding our ability to address a broad spectrum of national statistical inquiries.
Some of the complexities of this file are as follows. A lot of information in this file comes from extensive use of tables that are in non-standard format with nested hierarchies which can span multiple pages, but also from charts and normal text. PDF file is written in bilingual style, interlacing Czech and English even in the same visuals. Many tables contain footer information with additional descriptions about the values included. File itself is quite big with over 800 pages. Details of one of possible ways that I used to deal with this file are described later in complexities section. 
Script that is used to prepare unstructured data for application use is called pdf_to_chromadb.py.  It is used in several ways and can be used for experimentation. At the beginning of the script there are configuration variables, specifying which functionality to enable. Normally it runs in sequence, parsing one or more pdf files in parallel, chunking and storing in ChromaDB and finally testing retrieval on some test queries using semantic search functions. 
There are many different libraries for document parsing, but I have chosen the LLM parsing using LlamaParse service because of the stated complexities. Parsing is done using instructions provided to LLM. Those can be tweaked in the same script, turning off the chunking and testing part. Parsing creates a txt file with the same name as PDF file.
When we are done with parsing, we can proceed with chunking and testing, turning off the parsing step. Chunking can be done in many ways and tweaked using many hyperparameters until best aggregated search score is achieved, more on this later. Main variables to adjust here in our case are chunk size, overlap size, and embedding model used. 
After chunking is done and ChromaDB vector database is filled we can proceed with tweaking of search functionality turning off parsing and chunking. Strategies I used here to get most relevant chunks for our query are hybrid search using combination of semantic search with keyword search and reranking using Cohere’s Rerank Model. Hyperparameters used here are number of documents retrieved by hybrid search, weights for hybrid search – giving more importance to semantic part, and finally number of best matched documents returned as a result. Same script is also used as a module to be loaded into our main application to provide search functions over the vector database. 
