#!/usr/bin/env python3
"""
Test for Phase 12: Performance Testing
Based on test_concurrency.py pattern - tests application startup time and performance
"""

import os

# CRITICAL: Set Windows event loop policy FIRST, before other imports
import sys

if sys.platform == "win32":
    import asyncio

    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

# Load environment variables early
from dotenv import load_dotenv

load_dotenv()

# Standard imports
import asyncio
import time
import traceback
from datetime import datetime, timedelta
from pathlib import Path

import httpx
import psutil

# Add project root to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

# Import functionality from main scripts
from other.tests.test_concurrency import (
    check_server_connectivity,
    create_test_jwt_token,
)

# Test configuration
SERVER_BASE_URL = "http://localhost:8000"
REQUEST_TIMEOUT = 60.0


class PerformanceResults:
    """Class to track performance test results."""

    def __init__(self):
        self.startup_time = None
        self.memory_baseline = None
        self.memory_after_requests = None
        self.request_times = []
        self.concurrent_performance = None

    def add_request_time(self, endpoint: str, response_time: float):
        """Add a request timing result."""
        self.request_times.append(
            {
                "endpoint": endpoint,
                "response_time": response_time,
                "timestamp": datetime.now().isoformat(),
            }
        )

    def get_summary(self):
        """Get performance summary."""
        if self.request_times:
            avg_time = sum(r["response_time"] for r in self.request_times) / len(
                self.request_times
            )
            max_time = max(r["response_time"] for r in self.request_times)
            min_time = min(r["response_time"] for r in self.request_times)
        else:
            avg_time = max_time = min_time = 0

        return {
            "startup_time": self.startup_time,
            "memory_baseline_mb": self.memory_baseline,
            "memory_after_requests_mb": self.memory_after_requests,
            "memory_growth_mb": (
                (self.memory_after_requests - self.memory_baseline)
                if both_exist(self.memory_baseline, self.memory_after_requests)
                else None
            ),
            "total_requests": len(self.request_times),
            "avg_response_time": avg_time,
            "max_response_time": max_time,
            "min_response_time": min_time,
            "concurrent_performance": self.concurrent_performance,
        }


def both_exist(a, b):
    """Helper to check if both values exist."""
    return a is not None and b is not None


def get_memory_usage():
    """Get current memory usage in MB."""
    try:
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024
    except:
        return None


async def test_application_startup_time():
    """Test application startup time with modular structure."""
    print("üîç Testing application startup time...")

    try:
        # Measure time to get first successful health check
        start_time = time.time()

        # Wait for server to be ready (with timeout)
        max_wait = 30  # seconds
        while time.time() - start_time < max_wait:
            try:
                async with httpx.AsyncClient(timeout=httpx.Timeout(5.0)) as client:
                    response = await client.get(f"{SERVER_BASE_URL}/health")
                    if response.status_code == 200:
                        startup_time = time.time() - start_time
                        print(f"‚úÖ Application startup time: {startup_time:.2f}s")
                        return startup_time
            except:
                await asyncio.sleep(1)

        print("‚ùå Application did not start within timeout")
        return None

    except Exception as e:
        print(f"‚ùå Startup time test failed: {e}")
        return None


async def test_memory_usage_patterns():
    """Test memory usage patterns with modular structure."""
    print("üîç Testing memory usage patterns...")

    try:
        # Get baseline memory
        baseline_memory = get_memory_usage()
        print(
            f"üìä Baseline memory: {baseline_memory:.1f}MB"
            if baseline_memory
            else "üìä Could not measure baseline memory"
        )

        # Make several requests to test memory behavior
        token = create_test_jwt_token()
        headers = {"Authorization": f"Bearer {token}"}

        async with httpx.AsyncClient(
            base_url=SERVER_BASE_URL, timeout=httpx.Timeout(REQUEST_TIMEOUT)
        ) as client:
            # Make multiple requests to different endpoints
            endpoints = [
                "/health",
                "/health/memory",
                "/catalog",
                "/data-tables",
                "/chat-threads",
            ]

            for endpoint in endpoints:
                response = await client.get(
                    endpoint, headers=headers if endpoint != "/health" else None
                )
                print(f"üìä {endpoint}: {response.status_code}")
                await asyncio.sleep(0.5)  # Small delay between requests

        # Get memory after requests
        final_memory = get_memory_usage()
        print(
            f"üìä Memory after requests: {final_memory:.1f}MB"
            if final_memory
            else "üìä Could not measure final memory"
        )

        if baseline_memory and final_memory:
            growth = final_memory - baseline_memory
            print(f"üìä Memory growth: {growth:.1f}MB")

            # Check if memory growth is reasonable (less than 100MB for basic requests)
            if growth < 100:
                print("‚úÖ Memory usage is reasonable")
                return {
                    "baseline": baseline_memory,
                    "final": final_memory,
                    "growth": growth,
                }
            else:
                print("‚ö†Ô∏è High memory growth detected")
                return {
                    "baseline": baseline_memory,
                    "final": final_memory,
                    "growth": growth,
                }
        else:
            print("‚ö†Ô∏è Could not measure memory properly")
            return {"baseline": baseline_memory, "final": final_memory, "growth": None}

    except Exception as e:
        print(f"‚ùå Memory usage test failed: {e}")
        return None


async def test_concurrent_request_handling():
    """Test concurrent request handling performance."""
    print("üîç Testing concurrent request handling...")

    try:
        token = create_test_jwt_token()
        headers = {"Authorization": f"Bearer {token}"}

        async def make_request(endpoint: str, client: httpx.AsyncClient):
            """Make a single request and measure time."""
            start_time = time.time()
            try:
                response = await client.get(endpoint, headers=headers)
                response_time = time.time() - start_time
                return {
                    "endpoint": endpoint,
                    "status_code": response.status_code,
                    "response_time": response_time,
                    "success": response.status_code == 200,
                }
            except Exception as e:
                response_time = time.time() - start_time
                return {
                    "endpoint": endpoint,
                    "status_code": 0,
                    "response_time": response_time,
                    "success": False,
                    "error": str(e),
                }

        # Test concurrent requests to health endpoints
        concurrent_start = time.time()

        async with httpx.AsyncClient(
            base_url=SERVER_BASE_URL, timeout=httpx.Timeout(REQUEST_TIMEOUT)
        ) as client:
            # Make 5 concurrent requests
            tasks = [
                make_request("/health", client),
                make_request("/health/memory", client),
                make_request("/health/database", client),
                make_request("/health/rate-limits", client),
                make_request("/debug/pool-status", client),
            ]

            results = await asyncio.gather(*tasks, return_exceptions=True)

        concurrent_time = time.time() - concurrent_start

        # Analyze results
        successful_requests = sum(
            1 for r in results if isinstance(r, dict) and r.get("success", False)
        )
        total_requests = len(results)
        avg_response_time = (
            sum(r.get("response_time", 0) for r in results if isinstance(r, dict))
            / total_requests
        )

        print(f"üìä Concurrent test: {successful_requests}/{total_requests} successful")
        print(f"üìä Total concurrent time: {concurrent_time:.2f}s")
        print(f"üìä Average response time: {avg_response_time:.2f}s")

        performance_data = {
            "total_time": concurrent_time,
            "successful_requests": successful_requests,
            "total_requests": total_requests,
            "avg_response_time": avg_response_time,
            "success_rate": (successful_requests / total_requests) * 100,
        }

        if successful_requests >= 4:  # At least 80% success rate
            print("‚úÖ Concurrent request handling working well")
        else:
            print("‚ö†Ô∏è Some issues with concurrent request handling")

        return performance_data

    except Exception as e:
        print(f"‚ùå Concurrent request test failed: {e}")
        return None


async def test_endpoint_response_times():
    """Test individual endpoint response times."""
    print("üîç Testing endpoint response times...")

    results = PerformanceResults()

    try:
        token = create_test_jwt_token()
        headers = {"Authorization": f"Bearer {token}"}

        # Test endpoints with expected performance characteristics
        test_endpoints = [
            ("/health", None),  # Should be very fast
            ("/health/memory", None),  # Should be fast
            ("/catalog", headers),  # Should be moderate
            ("/data-tables", headers),  # Should be moderate
            ("/chat-threads", headers),  # Should be moderate
        ]

        async with httpx.AsyncClient(
            base_url=SERVER_BASE_URL, timeout=httpx.Timeout(REQUEST_TIMEOUT)
        ) as client:
            for endpoint, request_headers in test_endpoints:
                start_time = time.time()
                try:
                    response = await client.get(endpoint, headers=request_headers)
                    response_time = time.time() - start_time

                    results.add_request_time(endpoint, response_time)

                    status = "‚úÖ" if response.status_code == 200 else "‚ö†Ô∏è"
                    print(
                        f"{status} {endpoint}: {response_time:.3f}s (status: {response.status_code})"
                    )

                except Exception as e:
                    response_time = time.time() - start_time
                    results.add_request_time(endpoint, response_time)
                    print(f"‚ùå {endpoint}: {response_time:.3f}s (error: {str(e)[:50]})")

                # Small delay between requests
                await asyncio.sleep(0.2)

        return results

    except Exception as e:
        print(f"‚ùå Response time test failed: {e}")
        return results


async def main():
    """Main performance test runner."""
    print("üöÄ Phase 12: Performance Testing Starting...")
    print("=" * 60)

    # Check server connectivity first
    if not await check_server_connectivity():
        print("‚ùå Server connectivity check failed!")
        print("   Please start your uvicorn server first:")
        print(f"   uvicorn api.main:app --host 0.0.0.0 --port 8000")
        return False

    print("‚úÖ Server is accessible - running performance tests...")

    performance_results = PerformanceResults()

    # Test 1: Application startup time
    print(f"\nüìã Test 1: Application Startup Time")
    startup_time = await test_application_startup_time()
    performance_results.startup_time = startup_time

    # Test 2: Memory usage patterns
    print(f"\nüìã Test 2: Memory Usage Patterns")
    memory_data = await test_memory_usage_patterns()
    if memory_data:
        performance_results.memory_baseline = memory_data.get("baseline")
        performance_results.memory_after_requests = memory_data.get("final")

    # Test 3: Endpoint response times
    print(f"\nüìã Test 3: Endpoint Response Times")
    response_time_results = await test_endpoint_response_times()
    performance_results.request_times = response_time_results.request_times

    # Test 4: Concurrent request handling
    print(f"\nüìã Test 4: Concurrent Request Handling")
    concurrent_data = await test_concurrent_request_handling()
    performance_results.concurrent_performance = concurrent_data

    # Performance Analysis
    print("\n" + "=" * 60)
    print("üìä PERFORMANCE TEST RESULTS")
    print("=" * 60)

    summary = performance_results.get_summary()

    # Startup time analysis
    if summary["startup_time"]:
        startup_status = (
            "‚úÖ GOOD"
            if summary["startup_time"] < 10
            else "‚ö†Ô∏è SLOW" if summary["startup_time"] < 30 else "‚ùå TOO SLOW"
        )
        print(f"üöÄ Startup Time: {summary['startup_time']:.2f}s {startup_status}")

    # Memory analysis
    if summary["memory_growth_mb"]:
        memory_status = (
            "‚úÖ GOOD"
            if summary["memory_growth_mb"] < 50
            else "‚ö†Ô∏è HIGH" if summary["memory_growth_mb"] < 100 else "‚ùå EXCESSIVE"
        )
        print(f"üíæ Memory Growth: {summary['memory_growth_mb']:.1f}MB {memory_status}")

    # Response time analysis
    if summary["avg_response_time"]:
        response_status = (
            "‚úÖ GOOD"
            if summary["avg_response_time"] < 1.0
            else "‚ö†Ô∏è SLOW" if summary["avg_response_time"] < 3.0 else "‚ùå TOO SLOW"
        )
        print(
            f"‚ö° Avg Response Time: {summary['avg_response_time']:.3f}s {response_status}"
        )

    # Concurrent performance analysis
    if summary["concurrent_performance"]:
        concurrent_data = summary["concurrent_performance"]
        success_rate = concurrent_data.get("success_rate", 0)
        concurrent_status = (
            "‚úÖ GOOD"
            if success_rate >= 80
            else "‚ö†Ô∏è ISSUES" if success_rate >= 60 else "‚ùå POOR"
        )
        print(f"üîÑ Concurrent Success Rate: {success_rate:.1f}% {concurrent_status}")

    # Overall assessment
    issues = 0
    if summary["startup_time"] and summary["startup_time"] > 30:
        issues += 1
    if summary["memory_growth_mb"] and summary["memory_growth_mb"] > 100:
        issues += 1
    if summary["avg_response_time"] and summary["avg_response_time"] > 3.0:
        issues += 1
    if (
        summary["concurrent_performance"]
        and summary["concurrent_performance"].get("success_rate", 0) < 80
    ):
        issues += 1

    print(f"\nüèÅ OVERALL PERFORMANCE: ", end="")
    if issues == 0:
        print("üéâ EXCELLENT - All performance metrics are good")
        overall_success = True
    elif issues <= 1:
        print("‚úÖ GOOD - Minor performance issues detected")
        overall_success = True
    else:
        print(f"‚ö†Ô∏è NEEDS ATTENTION - {issues} performance issues detected")
        overall_success = False

    return overall_success


if __name__ == "__main__":
    # Set debug mode for better visibility
    os.environ["DEBUG"] = "1"
    os.environ["USE_TEST_TOKENS"] = "1"

    try:
        result = asyncio.run(main())
        sys.exit(0 if result else 1)
    except Exception as e:
        print(f"\nüí• Fatal error: {str(e)}")
        traceback.print_exc()
        sys.exit(1)
