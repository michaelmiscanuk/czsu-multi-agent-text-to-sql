{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7299e79",
   "metadata": {},
   "source": [
    "# Debugging Phoenix Evaluations\n",
    "\n",
    "This notebook helps diagnose and fix common issues with Phoenix evaluations, focusing particularly on QA evaluations using Azure OpenAI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ce25f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\retko\\anaconda3\\envs\\czsu-multi-agent-text-to-sqla\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Apply nest_asyncio early to avoid asyncio issues\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from phoenix.evals import OpenAIModel, QA_PROMPT_TEMPLATE, QA_PROMPT_RAILS_MAP, llm_classify\n",
    "from phoenix.experiments.types import EvaluationResult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc03aa4",
   "metadata": {},
   "source": [
    "## 1. Inspect QA Template & Variables\n",
    "\n",
    "First, let's examine the template to understand exactly what variables it expects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33c125be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given a question, an answer and reference text. You must determine whether the\n",
      "given answer correctly answers the question based on the reference text. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: {input}\n",
      "    ************\n",
      "    [Reference]: {reference}\n",
      "    ************\n",
      "    [Answer]: {output}\n",
      "    [END DATA]\n",
      "Your response must be a single word, either \"correct\" or \"incorrect\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"correct\" means that the question is correctly and fully answered by the answer.\n",
      "\"incorrect\" means that the question is not correctly or only partially answered by the\n",
      "answer.\n",
      "\n",
      "\n",
      "Extracted template variables: ['input', 'output', 'reference']\n",
      "\n",
      "Rails (expected output values): ['correct', 'incorrect']\n"
     ]
    }
   ],
   "source": [
    "# Print the template to clearly see expected variables\n",
    "import re\n",
    "\n",
    "template_text = str(QA_PROMPT_TEMPLATE)\n",
    "print(template_text)\n",
    "\n",
    "# Dynamically extract `{var}` placeholders from the template\n",
    "template_vars = sorted(set(re.findall(r\"\\{(\\w+)\\}\", template_text)))\n",
    "print(\"\\nExtracted template variables:\", template_vars)\n",
    "\n",
    "print(\"\\nRails (expected output values):\", list(QA_PROMPT_RAILS_MAP.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92759c5b",
   "metadata": {},
   "source": [
    "## 2. Test with a Single Example\n",
    "\n",
    "Testing with a single example helps isolate issues and verify the basic functionality works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98f05472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "input",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "reference",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "output",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "04dec30c-9014-4f88-a3eb-6a2f3895418f",
       "rows": [
        [
         "0",
         "What is the amount of men in Prague at the end of Q3 2024?",
         "676069",
         "Based on our data, there are 676,069 men living in Prague at the end of Q3 2024."
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>reference</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the amount of men in Prague at the end...</td>\n",
       "      <td>676069</td>\n",
       "      <td>Based on our data, there are 676,069 men livin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input reference  \\\n",
       "0  What is the amount of men in Prague at the end...    676069   \n",
       "\n",
       "                                              output  \n",
       "0  Based on our data, there are 676,069 men livin...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data with proper column names matching template variables\n",
    "test_data = pd.DataFrame({\n",
    "    \"input\": [\"What is the amount of men in Prague at the end of Q3 2024?\"],\n",
    "    \"reference\": [\"676069\"],\n",
    "    \"output\": [\"Based on our data, there are 676,069 men living in Prague at the end of Q3 2024.\"]\n",
    "})\n",
    "\n",
    "# Display the test data\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d39092",
   "metadata": {},
   "source": [
    "## 3. Test Azure OpenAI Configuration\n",
    "\n",
    "Make sure your Azure OpenAI configuration works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9028674a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test: OK\n",
      "‚úÖ Azure OpenAI connection successful\n"
     ]
    }
   ],
   "source": [
    "# Configure the Azure OpenAI Model\n",
    "model = OpenAIModel(\n",
    "    model=\"gpt-4o__test1\",  # For Azure, this is the deployment name\n",
    "    api_version=\"2024-05-01-preview\", \n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# Test the model with a simple query\n",
    "try:\n",
    "    response = model(\"Hello, please respond with 'OK' if you're working correctly.\")\n",
    "    print(\"Model test:\", response)\n",
    "    print(\"‚úÖ Azure OpenAI connection successful\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Azure OpenAI connection failed:\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f953e",
   "metadata": {},
   "source": [
    "## 4. Manual QA Evaluation Test\n",
    "\n",
    "Run a single evaluation to verify the functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f47cdd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.43s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "     label                                        explanation exceptions  \\\n",
      "0  correct  To determine if the answer is correct, we comp...         []   \n",
      "\n",
      "  execution_status  execution_seconds  \n",
      "0        COMPLETED           1.433905  \n",
      "\n",
      "Expected rails: ['correct', 'incorrect']\n",
      "\n",
      "Actual label: correct\n",
      "\n",
      "Explanation: To determine if the answer is correct, we compare the information provided in the answer with the reference text. The question asks for the number of men in Prague at the end of Q3 2024. The reference text states the number as 676069. The answer provided states that there are 676,069 men living in Prague at the end of Q3 2024. Both the numerical value and the context match exactly. Therefore, the answer correctly answers the question based on the reference text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rails = list(QA_PROMPT_RAILS_MAP.values())\n",
    "\n",
    "try:\n",
    "    # Run classification on single example\n",
    "    eval_results = llm_classify(\n",
    "        data=test_data,  # Use 'data' not 'dataframe'\n",
    "        template=QA_PROMPT_TEMPLATE,\n",
    "        model=model,\n",
    "        rails=rails,\n",
    "        provide_explanation=True\n",
    "    )\n",
    "    \n",
    "    # Display the results\n",
    "    print(\"Evaluation results:\")\n",
    "    print(eval_results)\n",
    "    print(\"\\nExpected rails:\", rails)\n",
    "    print(\"\\nActual label:\", eval_results[\"label\"].iloc[0])\n",
    "    print(\"\\nExplanation:\", eval_results[\"explanation\"].iloc[0])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Evaluation failed with error: {e}\")\n",
    "    \n",
    "    # Try to get more detailed error information\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3e6633",
   "metadata": {},
   "source": [
    "## 5. Implement QA Evaluator Function\n",
    "\n",
    "Test the evaluator function in isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b40424ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation DataFrame:\n",
      "                                               input reference  \\\n",
      "0  What is the amount of men in Prague at the end...    676069   \n",
      "\n",
      "                                              output  \n",
      "0  Based on our data, there are 676,069 men livin...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.42s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation successful: correct (score=1)\n",
      "Explanation: To determine if the answer is correct, we compare the information provided in the answer with the reference text. The question asks for the number of men in Prague at the end of Q3 2024. The reference text states the number as 676069. The answer states that there are 676,069 men living in Prague at the end of Q3 2024. Both the numerical value and the context match exactly. Therefore, the answer correctly answers the question based on the reference text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def qa_test_evaluator(output, reference):\n",
    "    \"\"\"LLM‚Äëbased evaluator that returns a Phoenix EvaluationResult.\"\"\"\n",
    "    from phoenix.experiments.types import EvaluationResult  # local import avoids circular issues\n",
    "\n",
    "    answer = str(output.get(\"results\", \"Based on our data, there are 676,069 men living in Prague at the end of Q3 2024.\"))\n",
    "    question = output.get(\"query\", \"What is the amount of men in Prague at the end of Q3 2024?\")\n",
    "    context = str(reference.get(\"context\", \"676069\"))\n",
    "\n",
    "    # Build DataFrame expected by QA_PROMPT_TEMPLATE\n",
    "    df_eval = pd.DataFrame(\n",
    "        {\"input\": [question], \"reference\": [context], \"output\": [answer]}\n",
    "    )\n",
    "    print(\"Evaluation DataFrame:\")\n",
    "    print(df_eval)\n",
    "\n",
    "    try:\n",
    "        eval_df = llm_classify(\n",
    "            data=df_eval,\n",
    "            template=QA_PROMPT_TEMPLATE,\n",
    "            model=model,\n",
    "            rails=rails,\n",
    "            provide_explanation=True,\n",
    "        )\n",
    "        label = eval_df[\"label\"].iloc[0]\n",
    "        score = 1 if label == \"correct\" else 0\n",
    "        explanation = eval_df[\"explanation\"].iloc[0]\n",
    "\n",
    "        print(f\"Evaluation successful: {label} (score={score})\")\n",
    "        print(f\"Explanation: {explanation}\")\n",
    "\n",
    "        return EvaluationResult(label=label, score=score, explanation=explanation)\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation failed: {e}\")\n",
    "        return EvaluationResult(\n",
    "            label=\"error\",\n",
    "            score=0,\n",
    "            explanation=f\"Error during evaluation: {e}\",\n",
    "        )\n",
    "\n",
    "# Test the evaluator with dummy data\n",
    "test_output = {\"query\": \"What is the amount of men in Prague at the end of Q3 2024?\", \"results\": \"Based on our data, there are 676,069 men living in Prague at the end of Q3 2024.\"}\n",
    "test_reference = {\"context\": \"676069\"}\n",
    "\n",
    "result = qa_test_evaluator(test_output, test_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ac0aba",
   "metadata": {},
   "source": [
    "## 6. Finding Experiment Issues in Phoenix\n",
    "\n",
    "The `run_experiment` function might fail if there are event loop issues. Here's a simpler approach that might be more reliable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b641cd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading dataset...\n",
      "üíæ Examples uploaded: https://app.phoenix.arize.com/datasets/RGF0YXNldDoyOQ==/examples\n",
      "üóÑÔ∏è Dataset version ID: RGF0YXNldFZlcnNpb246MzQ=\n",
      "Created test dataset with 1 records\n"
     ]
    }
   ],
   "source": [
    "from phoenix.session.client import Client\n",
    "\n",
    "# Initialize Phoenix client\n",
    "px_client = Client(warn_if_server_not_running=True)\n",
    "\n",
    "# Convert to smaller test set\n",
    "test_ground_truth = {\n",
    "    \"What is the amount of men in Prague at the end of Q3 2024?\": \"676069\",\n",
    "}\n",
    "test_df = pd.DataFrame(test_ground_truth.items(), columns=[\"question\", \"context\"])\n",
    "\n",
    "# Create a simplified dataset just for testing\n",
    "try:\n",
    "    test_dataset = px_client.upload_dataset(\n",
    "        dataset_name=f\"test_dataset_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "        dataframe=test_df,\n",
    "        input_keys=[\"question\"],\n",
    "        output_keys=[\"context\"]\n",
    "    )\n",
    "    print(f\"Created test dataset with {len(test_df)} records\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9de3709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: https://app.phoenix.arize.com/datasets/RGF0YXNldDoyOQ==/experiments\n",
      "üîó View this experiment: https://app.phoenix.arize.com/datasets/RGF0YXNldDoyOQ==/compare?experimentId=RXhwZXJpbWVudDo3Mw==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.35s/it\n",
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.35s/it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "üß† Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |          | 0/1 (0.0%) | ‚è≥ 00:00<? | ?it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation DataFrame:\n",
      "                                               input reference  \\\n",
      "0  What is the amount of men in Prague at the end...    676069   \n",
      "\n",
      "                                              output  \n",
      "0  Based on our data, the answer to 'What is the ...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.31s/it\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation successful: correct (score=1)\n",
      "Explanation: To determine if the answer is correct, we first examine the question, which asks for the amount of men in Prague at the end of Q3 2024. The reference text provides the number 676069. The answer states that the amount of men in Prague at the end of Q3 2024 is 676069, which matches the reference text exactly. Therefore, the answer correctly and fully addresses the question.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.79s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó View this experiment: https://app.phoenix.arize.com/datasets/RGF0YXNldDoyOQ==/compare?experimentId=RXhwZXJpbWVudDo3Mw==\n",
      "\n",
      "Experiment Summary (04/27/25 08:50 PM +0200)\n",
      "--------------------------------------------\n",
      "           evaluator  n  n_scores  avg_score  n_labels    top_2_labels\n",
      "0  qa_test_evaluator  1         1        1.0         1  {'correct': 1}\n",
      "\n",
      "Tasks Summary (04/27/25 08:50 PM +0200)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0           1       1         0\n",
      "Experiment completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Implement a simpler synchronous task function just for testing\n",
    "def simple_task(input_data):\n",
    "    \"\"\"A simplified task function for testing\"\"\"\n",
    "    question = input_data[\"question\"]\n",
    "    # Return a predefined result instead of calling main()\n",
    "    return {\n",
    "        \"query\": question,\n",
    "        \"results\": f\"Based on our data, the answer to '{question}' is 676069.\",\n",
    "        \"error\": None\n",
    "    }\n",
    "\n",
    "# Remove wrapper ‚Äì use the evaluator function directly\n",
    "# evaluator = create_evaluator(qa_test_evaluator)\n",
    "\n",
    "try:\n",
    "    from phoenix.experiments import run_experiment\n",
    "    simple_experiment = run_experiment(\n",
    "        dataset=test_dataset,\n",
    "        task=simple_task,\n",
    "        evaluators=[qa_test_evaluator],  # pass function directly\n",
    "        experiment_name=f\"simple_test_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    )\n",
    "    print(\"Experiment completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error running experiment: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "czsu-multi-agent-text-to-sqla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
